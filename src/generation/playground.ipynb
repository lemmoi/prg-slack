{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_messages = []\n",
    "with open('rand_messages.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        rand_messages.append(line)\n",
    "gen_messages = []\n",
    "with open('gen_messages.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        gen_messages.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coco: I am wondering if you guys can understand this..\\ncoco: Believe it or not.. it\\'s english\\nmaneki_neko: Happy New Year everyone!\\npfaendtner: Was the PRG new years resolution to abandon Slack?! :wink:\\nluizoliveira: Funny enough, that’s one of my resolutions, haha. I’m gonna use it only during the mornings when I’m commuting to work and in the night, at home. If I don’t promptly reply, that’s the reason.\\npfaendtner: this makes me unreasonably happy\\npfaendtner: Can anyone subscribe this channel to Logan Paul’s vlogs so they appear here automatically?\\ncnyambr: It is the third day since Oregonians have started pumping their own gas  Lolololol\\nanotherjoshsmith: wesleybeckner A quantitative description of the nontrivial difference between friends and colleagues. \\narushi: \"We’re getting dumber while our phones, cars, and homes are getting smarter.\"\\npfaendtner:'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_random_input_str(msgs):\n",
    "    start = np.random.randint(len(msgs)-10)\n",
    "    valid_msgs = 0\n",
    "    input_str = ''\n",
    "    while valid_msgs < 10:\n",
    "        msg = msgs[start]\n",
    "        if '<missing_message>' not in msg:\n",
    "            input_str += msg\n",
    "            valid_msgs += 1\n",
    "        else:\n",
    "            pass\n",
    "        start += 1\n",
    "    input_str += 'pfaendtner:'\n",
    "    return input_str\n",
    "\n",
    "inputs = make_random_input_str(rand_messages)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = tokenizer.encode(\"[Scene: Central Perk, Ross, Chandler, and Phoebe are there. Joey is working.]\\nRoss: Hey, remember when I had a monkey?\\nChandler: Yeah.\\nRoss: Yeah, what, what was I thinking?\",\n",
    "#                              return_tensors='pt')\n",
    "# input_ids = tokenizer.encode(\"The special problem we tried to get at with these lectures was to maintain the interest of the very enthusiastic and rather smart students coming out of the high schools and into Caltech. They have heard a lot about how interesting and exciting physics is—the theory of relativity, quantum mechanics, and other modern ideas. By the end of two years of our previous course, many would be very discouraged because there were really very few grand, new, modern ideas presented to them. They were made to study inclined planes, electrostatics, and so forth, and after two years it was quite stultifying. The problem was whether or not we could make a course which would save the more advanced and excited student by maintaining his enthusiasm.\",\n",
    "#                              return_tensors='pt')\n",
    "# input_ids = tokenizer.encode(\"I have a friend who's an artist and has sometimes taken a view which I don't agree with very well. He'll hold up a flower and say 'look how beautiful it is', and I'll agree. Then he says 'I as an artist can see how beautiful this is but you as a scientist take this all apart and it becomes a dull thing', and I think that he's kind of nutty. First of all, the beauty that he sees is available to other people and to me too, I believe. Although I may not be quite as refined aesthetically as he is ... I can appreciate the beauty of a flower. At the same time, I see much more about the flower than he sees. I could imagine the cells in there, the complicated actions inside, which also have a beauty.\",\n",
    "#                              return_tensors='pt')\n",
    "input_ids = tokenizer.encode(inputs,\n",
    "                             return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open-ended\n",
    "# sample_outputs = model.generate(\n",
    "#                     input_ids,\n",
    "#                     do_sample=True,\n",
    "#                     max_length=500,\n",
    "#                     top_k=75,\n",
    "#                     top_p=0.95,\n",
    "#                     num_return_sequences=5\n",
    "#                     )\n",
    "\n",
    "# single-response\n",
    "sample_outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    do_sample=True,\n",
    "                    eos_token_id=198,\n",
    "                    max_length=len(input_ids[0])+50,\n",
    "                    top_k=75,\n",
    "                    top_p=0.95,\n",
    "                    num_return_sequences=5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "coco: I am wondering if you guys can understand this..\n",
      "coco: Believe it or not.. it's english\n",
      "maneki_neko: Happy New Year everyone!\n",
      "pfaendtner: Was the PRG new years resolution to abandon Slack?! :wink:\n",
      "luizoliveira: Funny enough, that’s one of my resolutions, haha. I’m gonna use it only during the mornings when I’m commuting to work and in the night, at home. If I don’t promptly reply, that’s the reason.\n",
      "pfaendtner: this makes me unreasonably happy\n",
      "pfaendtner: Can anyone subscribe this channel to Logan Paul’s vlogs so they appear here automatically?\n",
      "cnyambr: It is the third day since Oregonians have started pumping their own gas  Lolololol\n",
      "anotherjoshsmith: wesleybeckner A quantitative description of the nontrivial difference between friends and colleagues. \n",
      "arushi: \"We’re getting dumber while our phones, cars, and homes are getting smarter.\"\n",
      "\n",
      "Responses\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "gpt-jim: i would be a lukewarm-to-warm fan\n",
      "\n",
      "\n",
      "gpt-jim: \"A smartwatch without apps is only smart\" - @TayandI\n",
      "\n",
      "\n",
      "gpt-jim: Thanks for being here!\n",
      "\n",
      "\n",
      "gpt-jim: i am so glad people don’t just buy a shit ton of things but at the same time are willing to purchase a shit ton of stuff.\n",
      "\n",
      "\n",
      "gpt-jim: What did he get from the NFP survey?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Context:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_outputs[0][:len(input_ids[0])-7],\n",
    "                       skip_special_tokens=True))\n",
    "print(\"\\nResponses\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}\".format(tokenizer.decode(sample_output[len(input_ids[0])-7:],\n",
    "                                              skip_special_tokens=True)).replace('pfaendtner', 'gpt-jim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slack Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('prg_msg.db')\n",
    "c = conn.cursor()\n",
    "tables = ['users', 'rand_messages', 'gen_messages']\n",
    "table_cols = []\n",
    "data = []\n",
    "for table in tables:\n",
    "    c.execute(\"select * from %s where 1=0;\" % table)\n",
    "    table_cols = [d[0] for d in c.description]\n",
    "    select_str = ''\n",
    "    for col in table_cols:\n",
    "        select_str += '{}, '.format(col)\n",
    "    select_str = select_str[:-2]\n",
    "    c.execute(\"select {} from {}\".format(select_str, table))\n",
    "    data.append(c.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = data[0]\n",
    "rand_messages = data[1]\n",
    "gen_messages = data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_dict = {}\n",
    "for user in users:\n",
    "    users_dict[user[0]] = user[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_message(msg, user_dict):\n",
    "    user = user_dict[msg[1]]\n",
    "    msg_text = msg[2]\n",
    "    if msg_text == '':\n",
    "        msg_text = '<missing_message>'\n",
    "    msg_out = '{}: {}'.format(user, msg_text)\n",
    "    return msg_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rand_messages.txt', 'w') as f:\n",
    "    for msg in reversed(rand_messages):\n",
    "        f.write('{}\\n'.format(decode_message(msg, users_dict)))\n",
    "\n",
    "with open('gen_messages.txt', 'w') as f:\n",
    "    for msg in reversed(gen_messages):\n",
    "        f.write('{}\\n'.format(decode_message(msg, users_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7297, 5498)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rand_messages), len(gen_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
